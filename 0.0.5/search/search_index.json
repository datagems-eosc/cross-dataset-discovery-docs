{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the documentation for the Cross-Dataset Discovery Service.</p> <p>This service provides an API for performing natural language search queries across a pre-indexed collection of datasets.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ul> <li>System: Learn about the high-level Architecture, Security model, and Datastores.</li> <li>API: Find detailed information on the API endpoints and error codes.</li> <li>Setup &amp; Monitoring: Get instructions on Deployment, Configuration, and Logging.</li> </ul>"},{"location":"api-overview/","title":"Overview","text":""},{"location":"api-overview/#api-overview","title":"API Overview","text":"<p>The Cross-Dataset Discovery service exposes a RESTful API for performing search operations.</p>"},{"location":"api-overview/#base-url","title":"Base URL","text":"<p>Base URL: https://datagems-dev.scayle.es/cross-dataset-discovery/</p>"},{"location":"api-overview/#authentication","title":"Authentication","text":"<p>All endpoints are protected and require a valid JWT Bearer token.</p>"},{"location":"api-overview/#endpoints","title":"Endpoints","text":""},{"location":"api-overview/#search","title":"Search","text":"<p>Performs a search query against the indexed datasets.</p> <ul> <li>Endpoint: <code>POST /search/</code></li> <li>Description: Submits a natural language query and returns a ranked list of relevant results. Supports Sparse (BM25), Dense (Semantic), and Hybrid search modes.</li> <li>Request Body:</li> </ul> <pre><code>{\n  \"query\": \"string\",\n  \"k\": 5,\n  \"dataset_ids\": [\n    \"string\"\n  ],\n  \"search_mode\": \"sparse\" \n}\n</code></pre> <ul> <li> <p><code>search_mode</code>: Optional. Can be <code>\"sparse\"</code> (default), <code>\"dense\"</code>, or <code>\"hybrid\"</code>.</p> </li> <li> <p>Response Body (Success):</p> </li> </ul> <pre><code>{\n  \"query_time\": \"retrieval time in milliseconds\",\n  \"results\": [\n    {\n      \"content\": \"The main text content of the search result.\",\n      \"dataset_id\": \"uuid-of-the-source-dataset\",\n      \"object_id\": \"uuid-of-the-source-object\",\n      \"similarity\": \"score (BM25 score, Cosine Similarity, or RRF score)\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api-overview/#health-check","title":"Health Check","text":"<p>Verifies the operational status of the API and its dependencies.</p> <ul> <li>Endpoint: <code>GET /health</code></li> <li>Description: Checks connectivity to the Database, Qdrant, and the TEI service.</li> <li>Response Body (Success):</li> </ul> <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"All dependencies are healthy.\"\n}\n</code></pre>"},{"location":"architecture/","title":"System Architecture","text":"<p>The Cross-Dataset Discovery Service is designed as a microservices-based system within the DataGEMS ecosystem. It acts as a specialized search layer that orchestrates retrieval across vector databases and inference services.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ol> <li> <p>FastAPI Application (API Pod): The entry point of the service. It handles authentication, authorization, and orchestrates the search logic. It calculates sparse embeddings locally using <code>FastEmbed</code> (CPU) to minimize latency.</p> </li> <li> <p>Vector Database (Qdrant Pod): A high-performance vector search engine. It stores both dense (semantic) and sparse (keyword) vectors for the dataset corpus and performs the core retrieval and ranking operations (including Hybrid Search via Reciprocal Rank Fusion).</p> </li> <li> <p>Embedding Service (TEI Pod): A dedicated Text Embeddings Inference service (based on Hugging Face TEI). It provides an API for generating dense vectors (using <code>BAAI/bge-m3</code>) from input text.</p> </li> <li> <p>Security Layer: Intercepts requests to validate OIDC tokens and communicates with the DataGEMS Gateway to enforce dataset-level access control.</p> </li> </ol>"},{"location":"architecture/#request-flow","title":"Request Flow","text":"<p>A typical search request follows this path:</p> <ol> <li>Auth: A user sends a <code>POST /search/</code> request with a valid JWT. The Security Layer validates the token and fetches authorized dataset IDs from the DataGEMS Gateway.</li> <li>Vector Generation:<ul> <li>Sparse Mode: The API Pod generates a sparse vector (BM25) locally.</li> <li>Dense Mode: The API Pod sends the query text to the TEI Service to get a dense vector.</li> <li>Hybrid Mode: Both operations are performed.</li> </ul> </li> <li>Retrieval: The API Pod sends the generated vector(s) and the list of authorized dataset IDs (as a filter) to Qdrant.</li> <li>Ranking: Qdrant performs the search (using HNSW for dense, Inverted Index for sparse, or RRF for hybrid) and returns the top results.</li> <li>Response: The API formats the results and returns them to the user.</li> </ol>"},{"location":"automations/","title":"Automations","text":"<p>This project leverages GitHub Actions to automate builds, code analysis, security scanning, and documentation deployment. The following sections detail the specific workflows configured for this repository.</p>"},{"location":"automations/#dockerfile","title":"Dockerfile","text":"<p>The creation of a production-ready container image is fully automated by the <code>Dockerfile</code> in the root of the repository. This build process ensures a consistent environment for the application.</p> <ul> <li>Stage 1 (Builder): This stage prepares the environment by installing all necessary build-time dependencies, including OpenJDK (for the search library) and the full set of Python packages.</li> <li>Stage 2 (Final): This stage constructs the final, lean image by copying only the essential artifacts from the builder stage. It creates a non-root <code>appuser</code> for security, copies the application code, and sets the <code>gunicorn</code> server as the entry point.</li> </ul>"},{"location":"automations/#docker-image-publishing","title":"Docker image publishing","text":"<p>Workflow: <code>.github/workflows/docker-publish.yml</code></p> <p>This workflow automates the process of building and publishing the service's Docker image to the GitHub Container Registry (ghcr.io).</p> <ul> <li>Trigger: This workflow runs automatically whenever a new Git tag matching the pattern <code>v*</code> (e.g., <code>v1.0.0</code>, <code>v1.1.0</code>) is pushed to the repository.</li> <li>Process:<ol> <li>Logs into the GitHub Container Registry using a temporary <code>GITHUB_TOKEN</code>.</li> <li>Uses the <code>docker/metadata-action</code> to automatically generate appropriate Docker tags based on the Git tag.</li> <li>Builds the Docker image using the <code>Dockerfile</code>.</li> <li>Pushes the newly built and tagged image to the <code>ghcr.io/datagems-eosc/cross-dataset-discovery-api</code> repository, making it available for deployment.</li> </ol> </li> </ul>"},{"location":"automations/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Workflow: <code>.github/workflows/vulnerability-scan-on-demand.yml</code></p> <p>This workflow provides on-demand security scanning of the project's configuration and Docker images using the Trivy security scanner.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>). It requires an <code>image_tag</code> as input to specify which published Docker image to scan.</li> <li>Process:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository configuration files for potential security misconfigurations.</li> <li>Image Scan: Pulls the specified Docker image from the GitHub Container Registry and scans its operating system packages and Python libraries for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity.</li> <li>Artifacts: The JSON reports from both the configuration scan and the image scan are uploaded as build artifacts for review.</li> </ol> </li> </ul>"},{"location":"automations/#static-code-analysis","title":"Static Code Analysis","text":"<p>Workflow: <code>.github/workflows/code-scan-on-demand.yml</code></p> <p>This workflow performs in-depth static code analysis using GitHub CodeQL to find potential bugs, security vulnerabilities, and quality issues in the codebase.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>It runs two separate analysis jobs: one for the Python source code and one for the GitHub Actions workflow files themselves.</li> <li>For each language, it initializes CodeQL using an extended set of security and quality queries.</li> <li>It performs the analysis and uploads the results directly to the repository's \"Security\" tab under \"Code scanning alerts\".</li> </ol> </li> </ul>"},{"location":"automations/#code-metrics","title":"Code Metrics","text":"<p>Workflow: <code>.github/workflows/code-metrics-on-demand.yml</code></p> <p>This workflow generates a report on the complexity and maintainability of the application's Python code using the Radon library.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>Installs the <code>radon</code> Python package.</li> <li>Runs three types of analysis on the <code>cross_dataset_discovery/api_datagems_cross_dataset_discovery/app</code> directory:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain.</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code, indicating its complexity.</li> <li>Raw Lines of Code (LOC): Provides basic code size metrics.</li> </ul> </li> <li>The complete report is compiled into a text file and uploaded as a build artifact named <code>code-metrics-report</code>.</li> </ol> </li> </ul>"},{"location":"automations/#documentation","title":"Documentation","text":"<p>Workflow: <code>.github/workflows/deploy-docs-on-demand.yml</code></p> <p>The deployment of this documentation site is automated by a dedicated GitHub Action workflow.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process: When triggered with a version number (e.g., <code>1.0.0</code>), the workflow:<ol> <li>Installs <code>mkdocs</code>, <code>mike</code>, and other required tools.</li> <li>Builds the static HTML site from the markdown source files in the <code>docs/</code> directory.</li> <li>Uses the <code>mike</code> tool to commit the built site to the <code>gh-pages</code> branch, organized under the specified version.</li> <li>Updates the <code>latest</code> alias to point to this newly deployed version, ensuring visitors see the most recent documentation by default.</li> </ol> </li> </ul>"},{"location":"configuration/","title":"Cross-Dataset Discovery Service Documentation","text":""},{"location":"configuration/#configuration","title":"Configuration","text":"<p>The Cross-Dataset Discovery service is configured using a combination of a local configuration file and environment variables.</p>"},{"location":"configuration/#configuration-files","title":"Configuration Files","text":"<p>For local development, the service can be configured using a <code>.env</code> file placed in the root of the repository.</p>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> file","text":"<pre><code># .env\n\n# OIDC Authentication\nOIDC_ISSUER_URL=\"https://datagems-dev.scayle.es/oauth/realms/dev\"\nOIDC_AUDIENCE=\"cross-dataset-discovery-api\"\nGATEWAY_API_URL=\"https://datagems-dev.scayle.es/gw\"\n\n# Database (PostgreSQL)\nDB_CONNECTION_STRING=\"postgresql://user:password@localhost:5432/datagems_db\"\nTABLE_NAME=\"your_table_name\"\n\n# Search Infrastructure\nQDRANT_URL=\"http://localhost:6333\"\nQDRANT_COLLECTION=\"datagems\"\nTEI_URL=\"http://localhost:8080\"\nTEI_TIMEOUT=2.0\n\n# Secrets (for local use only)\nIdpClientSecret=\"your-local-dev-secret\"\n</code></pre>"},{"location":"configuration/#environment-overrides","title":"Environment Overrides","text":"<p>In production (Kubernetes), environment variables override the <code>.env</code> file.</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider https://.../realms/dev OIDC_AUDIENCE The audience claim required in the JWT cross-dataset-discovery-api GATEWAY_API_URL The DataGEMS Gateway API URL for permissions https://.../gw DB_CONNECTION_STRING PostgreSQL connection string postgresql://... QDRANT_URL URL of the Qdrant Vector Database service http://qdrant-service:6333 QDRANT_COLLECTION The name of the Qdrant collection to search datagems TEI_URL URL of the Text Embeddings Inference service http://tei-service:80 TEI_TIMEOUT Timeout (in seconds) for embedding requests 2.0 IdpClientSecret Client secret for the identity provider your-secret"},{"location":"configuration/#secrets","title":"Secrets","text":"<ul> <li>DB_CONNECTION_STRING: Contains database credentials</li> <li>IdpClientSecret: Contains OIDC client secret</li> </ul>"},{"location":"datastore/","title":"Datastores","text":"<p>The Cross-Dataset Discovery service relies on a Vector Database for fast retrieval.</p>"},{"location":"datastore/#vector-database-qdrant","title":"Vector Database (Qdrant)","text":"<p>The core retrieval functionality is powered by Qdrant, a high-performance vector search engine.</p> <ul> <li>Technology: Qdrant</li> <li>Purpose: Stores embeddings and metadata for all datasets. Supports:<ul> <li>Dense Vectors: 1024-dimensional vectors (generated by <code>BAAI/bge-m3</code>) for semantic search.</li> <li>Sparse Vectors: Term-frequency vectors for keyword search (BM25).</li> <li>Hybrid Search: Combines dense and sparse results using Reciprocal Rank Fusion (RRF).</li> </ul> </li> <li>Location: Running as a separate service within the Kubernetes cluster. Data is persisted on a Persistent Volume Claim (PVC).</li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>The Cross-Dataset Discovery service is designed for containerized deployment using Docker.</p>"},{"location":"deployment/#docker","title":"Docker","text":"<p>The repository includes a <code>Dockerfile</code> for the build process.</p>"},{"location":"deployment/#dockerfile-stages","title":"Dockerfile Stages","text":"<ol> <li>Builder Stage:</li> <li>Starts from <code>python:3.11-slim</code>.</li> <li>Installs Python dependencies.</li> <li> <p>Pre-downloads the <code>Qdrant/bm25</code> model for FastEmbed to ensure fast startup.</p> </li> <li> <p>Final Stage:</p> </li> <li>Copies the application code and the pre-downloaded model cache.</li> <li>Runs as a non-root user (<code>appuser</code>).</li> <li>Exposes port <code>8000</code>.</li> </ol>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<p>To build and run the container:</p> <pre><code># 1. Build the Docker image\ndocker build -t cross-dataset-discovery .\n\n# 2. Run the container\n# Note: Requires network access to Qdrant and TEI services.\ndocker run -p 8000:8000 \\\n  -e DB_CONNECTION_STRING=\"postgresql://...\" \\\n  -e OIDC_ISSUER_URL=\"https://...\" \\\n  -e QDRANT_URL=\"http://host.docker.internal:6333\" \\\n  -e TEI_URL=\"http://host.docker.internal:8080\" \\\n  --name cdd-service \\\n  cross-dataset-discovery\n</code></pre>"},{"location":"deployment/#dependencies","title":"Dependencies","text":"<p>The service requires several external systems to function.</p>"},{"location":"deployment/#runtime-dependencies","title":"Runtime Dependencies","text":"<ol> <li>Qdrant Vector Database:</li> <li>Requirement: A running Qdrant instance containing the datagems collection.</li> <li> <p>Config: <code>QDRANT_URL</code>, <code>QDRANT_COLLECTION</code>.</p> </li> <li> <p>Text Embeddings Inference (TEI):</p> </li> <li>Requirement: A running TEI service serving the <code>BAAI/bge-m3</code> model.</li> <li> <p>Config: <code>TEI_URL</code>.</p> </li> <li> <p>OIDC Provider:</p> </li> <li> <p>Requirement: Keycloak (or similar) for JWT validation.</p> </li> <li> <p>DataGEMS Gateway:</p> </li> <li>Requirement: API access for fetching user permissions.</li> </ol>"},{"location":"deployment/#build-time-dependencies","title":"Build-time Dependencies","text":"<ul> <li>Python 3.11</li> <li>FastEmbed Model: The <code>Qdrant/bm25</code> model is downloaded during the Docker build to enable local sparse vector generation without runtime downloads.</li> </ul>"},{"location":"error-codes/","title":"Status &amp; Error Codes","text":"<p>The API uses standard HTTP status codes to indicate the success or failure of a request.</p> Status Code Meaning Description <code>200 OK</code> Success The request was successful. <code>400 Bad Request</code> Validation Error The request body is malformed or contains invalid data (e.g., <code>k</code> is out of range, <code>dataset_ids</code> is an empty list). The response body will contain details about the validation error. <code>401 Unauthorized</code> Authentication Error The request lacks a valid JWT, the token is expired, or its signature is invalid. <code>403 Forbidden</code> Authorization Error The user is authenticated but does not have the required role (<code>user</code> or <code>dg_user</code>) to perform the action. <code>424 Failed Dependency</code> Downstream Service Error The API could not communicate with a required dependency, such as the OIDC provider or the database. The response body will contain details about the source of the failure. <code>500 Internal Server Error</code> Unexpected Error An unexpected error occurred on the server while processing the request. <code>503 Service Unavailable</code> Service Not Ready A core component, like the searcher, failed to initialize at startup. This is typically seen during health checks."},{"location":"faq/","title":"FAQ & Known Issues","text":"<p>TODO</p>"},{"location":"license/","title":"License","text":"<p>The Cross-Dataset Discovery service is licensed under the MIT License.</p>"},{"location":"license/#mit-license","title":"MIT License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 DataGEMS EOSC\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>The Cross-Dataset Discovery service uses several third-party libraries, each with its own license. The major dependencies and their licenses are listed below:</p>"},{"location":"license/#python-libraries","title":"Python Libraries","text":"Library License FAISS MIT License FastAPI MIT License NLTK Apache 2.0 License Pandas BSD 3-Clause License Pydantic MIT License Pyserini Apache 2.0 License PyTorch PyTorch License Transformers Apache 2.0 License Uvicorn BSD 3-Clause License"},{"location":"license/#european-union-funding","title":"European Union Funding","text":"<p>DataGEMS is a Research and Innovation Action funded by the European Union under the Horizon Europe Research and Innovation Programme via Grant Agreement No 101188416.</p> <p></p>"},{"location":"license/#citing-cross-dataset-discovery","title":"Citing Cross-Dataset Discovery","text":"<p>If you use the Cross-Dataset Discovery service in your research, please cite it as follows:</p> <pre><code>DataGEMS EOSC. (2025). Cross-Dataset Discovery. GitHub. https://github.com/datagems-eosc/cross-dataset-discovery\n</code></pre> <p>BibTeX:</p> <pre><code>@software{cross-dataset-discovery,\n  author = {{DataGEMS EOSC}},\n  title = {Cross-Dataset Discovery},\n  year = {2025},\n  publisher = {GitHub},\n  url = {https://github.com/datagems-eosc/cross-dataset-discovery}\n}\n</code></pre>"},{"location":"license/#contact","title":"Contact","text":"<p>For questions regarding the license or usage of the Cross-Dataset Discovery service, please contact:</p> <ul> <li>GitHub: https://github.com/datagems-eosc/cross-dataset-discovery</li> <li>Email: contact@datagems.eu</li> </ul>"},{"location":"logging/","title":"Logging &amp; Accounting","text":"<p>The service uses <code>structlog</code> for structured, JSON-formatted logging.</p>"},{"location":"logging/#log-structure","title":"Log Structure","text":"<p>All log entries are formatted as JSON objects with a consistent structure, including:</p> <ul> <li><code>@t</code>: ISO 8601 timestamp.</li> <li><code>@mt</code>: The log message (event).</li> <li><code>@l</code>: The log level (e.g., \"Info\", \"Warning\", \"Error\").</li> <li><code>DGCorrelationId</code>: A unique ID that ties together all log entries for a single HTTP request.</li> <li><code>SourceContext</code>: The name of the logger that produced the message.</li> <li><code>Application</code>: The name of the application (<code>cross-dataset-discovery-api</code>).</li> </ul> <p>Additional key-value pairs are added to provide context for specific events.</p>"},{"location":"logging/#correlation-id","title":"Correlation ID","text":"<p>Every HTTP request is assigned a correlation ID. - If the incoming request includes an <code>x-tracking-correlation</code> header, its value is used. - Otherwise, a new UUID is generated.</p> <p>This ID is included in every log message generated during the processing of that request and is also returned in the <code>x-tracking-correlation</code> response header. This allows for easy tracing of a request's entire lifecycle through the system and across different services.</p>"},{"location":"logging/#request-logging","title":"Request Logging","text":"<p>A middleware automatically logs the end of every HTTP request (except for health checks), capturing:</p> <ul> <li><code>RequestMethod</code> (e.g., GET, POST)</li> <li><code>RequestPath</code></li> <li><code>StatusCode</code></li> <li><code>ProcessTimeMS</code></li> <li><code>ResponseSize</code></li> </ul>"},{"location":"maintenance/","title":"Maintenance","text":"<p>TODO</p>"},{"location":"maintenance/#re-indexing","title":"Re-indexing","text":"<p>A key maintenance task for this service is updating the search index.</p>"},{"location":"onboarding/","title":"Developer Onboarding Guide","text":"<p>Welcome to the developer guide for the <code>nlp_retrieval</code> framework.</p> <p>This document provides a comprehensive overview of the framework's architecture and a step-by-step guide for extending it with new components (like loaders, retrievers, or rerankers) and benchmarking their performance.</p>"},{"location":"onboarding/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Framework Overview</li> <li>Core Principles</li> <li>Directory Structure</li> <li>Core Data Models</li> <li>System Architecture &amp; Workflows</li> <li>Core Components &amp; Interfaces</li> <li>Indexing Workflow</li> <li>Search Workflow</li> <li>Part 1: Adding a New Component</li> <li>General Steps</li> <li>Component-Specific Guides</li> <li>Assembling a Pipeline</li> <li>Part 2: Evaluating Your New Component</li> <li>Key Evaluation Classes</li> <li>The Matching Logic</li> <li>How to Run a Benchmark</li> <li>Interpreting the Output</li> </ol>"},{"location":"onboarding/#1-framework-overview","title":"1. Framework Overview","text":"<p>The <code>nlp_retrieval</code> module provides a modular, configurable, and extensible framework for building and benchmarking end-to-end text retrieval pipelines. It is designed around a \"composition over inheritance\" model, where a central <code>Searcher</code> orchestrates various independent components to perform indexing and searching tasks.</p>"},{"location":"onboarding/#core-principles","title":"Core Principles","text":"<p>The framework is built on a few key principles:</p> <ul> <li>Modularity: Each component is a self-contained, swappable Python class.</li> <li>Clear Interfaces: Components inherit from an Abstract Base Class (ABC) that defines a strict contract for its methods.</li> <li>Standardized Data Flow: We use Pydantic models (<code>SearchableItem</code>, <code>RetrievalResult</code>) to pass data between components.</li> <li>Batch-First Design: All components are designed to operate on batches (<code>List</code>) of queries or results, enabling efficient processing.</li> </ul>"},{"location":"onboarding/#directory-structure","title":"Directory Structure","text":"<p>All retrieval-related code resides in <code>darelabdb/nlp_retrieval/</code>.</p> <pre><code>nlp_retrieval/\n\u251c\u2500\u2500 core/                      # Pydantic models &amp; ABCs\n\u251c\u2500\u2500 loaders/                   # Reads data from sources\n\u251c\u2500\u2500 user_query_processors/     # Processes raw user queries\n\u251c\u2500\u2500 retrievers/                # Indexes data and retrieves candidates\n\u251c\u2500\u2500 rerankers/                 # Re-scores and ranks candidates\n\u251c\u2500\u2500 evaluation/                # Tools for calculating metrics\n\u251c\u2500\u2500 benchmarking/              # Orchestrator for running benchmarks\n\u2514\u2500\u2500 searcher.py                # The main pipeline orchestrator\n</code></pre>"},{"location":"onboarding/#core-data-models","title":"Core Data Models","text":"<p>Before you start, familiarize yourself with the two fundamental data models defined in <code>darelabdb/nlp_retrieval/core/models.py</code>:</p> <ul> <li><code>SearchableItem</code>: The canonical representation of a single piece of data to be indexed.</li> <li><code>item_id: str</code>: A unique identifier for the item.</li> <li><code>content: str</code>: The main text content used for retrieval.</li> <li><code>metadata: Dict</code>: A dictionary for any associated metadata.</li> <li><code>RetrievalResult</code>: Represents a single item returned by a search.</li> <li><code>item: SearchableItem</code>: The retrieved item.</li> <li><code>score: float</code>: The relevance score assigned by the retriever or reranker.</li> </ul>"},{"location":"onboarding/#2-system-architecture-workflows","title":"2. System Architecture &amp; Workflows","text":"<p>The framework is built upon a set of abstract base classes (ABCs), each defining a specific role in the retrieval pipeline. The <code>Searcher</code> and <code>Benchmarker</code> classes orchestrate the interaction between these components.</p>"},{"location":"onboarding/#core-components-interfaces","title":"Core Components &amp; Interfaces","text":"<ul> <li><code>BaseLoader</code>: Responsible for loading data from a source (e.g., file, database) and converting it into a <code>List[SearchableItem]</code>.</li> <li><code>BaseUserQueryProcessor</code>: Transforms a batch of user queries into a format suitable for retrieval (e.g., decomposition, keyword extraction).</li> <li><code>BaseRetriever</code>: Manages the indexing of <code>SearchableItem</code>s and the retrieval of candidate results.</li> <li><code>BaseReranker</code>: Re-scores a list of candidate <code>RetrievalResult</code>s to improve ranking quality.</li> </ul>"},{"location":"onboarding/#indexing-workflow","title":"Indexing Workflow","text":"<p>The indexing process is initiated by the <code>Searcher.index()</code> method.</p> <ol> <li>Load: An instance of a <code>BaseLoader</code> (e.g., <code>JsonlLoader</code>) is used to load data from a source into a <code>List[SearchableItem]</code>.</li> <li>Index: The <code>Searcher</code> passes this list to the <code>index()</code> method of each configured <code>BaseRetriever</code> (e.g., <code>PyseriniRetriever</code>, <code>FaissRetriever</code>).</li> <li>Store: Each retriever builds its specific index (e.g., a Lucene index, a FAISS vector index) and saves it to a dedicated subdirectory.</li> </ol> <p> Figure 1: The indexing workflow showing data flow from source through loader to multiple retrievers creating their respective indexes.</p>"},{"location":"onboarding/#search-workflow","title":"Search Workflow","text":"<p>The search process is initiated by the <code>Searcher.search()</code> method.</p> <ol> <li>Process Query: The input queries are passed to the configured <code>BaseUserQueryProcessor</code>.</li> <li>Retrieve: The <code>Searcher</code> passes the processed queries to each configured <code>BaseRetriever</code>.</li> <li>Combine (Hybrid Search): The <code>Searcher</code> collects and deduplicates the results from all retrievers into a single pool of candidates.</li> <li>Rerank (Optional): If a <code>BaseReranker</code> is configured, this combined pool is passed to its <code>rerank()</code> method for re-scoring.</li> <li>Return: The final, sorted list of results is returned.</li> </ol> <p> Figure 2: The complete search pipeline from query input through processing, retrieval, combination, and optional reranking.</p>"},{"location":"onboarding/#3-part-1-adding-a-new-component","title":"3. Part 1: Adding a New Component","text":""},{"location":"onboarding/#general-steps","title":"General Steps","text":"<ol> <li>Identify the Component Type: Determine which category your new class falls into: <code>loaders</code>, <code>user_query_processors</code>, <code>retrievers</code>, or <code>rerankers</code>.</li> <li>Create the File: Add a new Python file in the appropriate directory (<code>cross_dataset_discovery/darelabdb/nlp_retrieval/retrievers/my_new_retriever.py</code>).</li> <li>Inherit from the ABC: Your new class must inherit from the correct Abstract Base Class (e.g., <code>BaseRetriever</code>).</li> <li>Implement All Abstract Methods: Your IDE or Python itself will require you to implement all methods marked with <code>@abstractmethod</code> in the parent ABC.</li> <li>Use Core Data Models: Your component must accept and/or return the Pydantic models (<code>SearchableItem</code>, <code>RetrievalResult</code>) as defined in the ABC contract.</li> </ol>"},{"location":"onboarding/#component-specific-guides","title":"Component-Specific Guides","text":""},{"location":"onboarding/#creating-a-new-loader","title":"Creating a New Loader","text":"<ul> <li>Inherit from: <code>BaseLoader</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Accept necessary parameters, like file paths.</li> <li><code>load(self) -&gt; List[SearchableItem]</code>: Read your data source and create a list of <code>SearchableItem</code> objects.</li> </ul>"},{"location":"onboarding/#creating-a-new-user-query-processor","title":"Creating a New User Query Processor","text":"<ul> <li>Inherit from: <code>BaseUserQueryProcessor</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Accept any configuration for your processing logic.</li> <li><code>process(self, nlqs: List[str]) -&gt; List[List[str]]</code>: Receives a list of raw queries and must return a list of lists, where each inner list contains the processed strings (e.g., sub-queries, keywords) for an input query.</li> </ul>"},{"location":"onboarding/#creating-a-new-retriever","title":"Creating a New Retriever","text":"<ul> <li>Inherit from: <code>BaseRetriever</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Initialize your retrieval model, parameters, etc.</li> <li><code>index(self, items: List[SearchableItem], output_path: str) -&gt; None</code>: Build your search index from the items and save all index files inside the provided <code>output_path</code>.</li> <li><code>retrieve(self, processed_queries_batch: List[List[str]], output_path: str, k: int) -&gt; List[List[RetrievalResult]]</code>: Load the index and process the batch of sub-queries, returning a single, aggregated, and deduplicated list of <code>RetrievalResult</code> objects for each original query.</li> </ul>"},{"location":"onboarding/#creating-a-new-reranker","title":"Creating a New Reranker","text":"<ul> <li>Inherit from: <code>BaseReranker</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Initialize your reranking model.</li> <li><code>rerank(self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int) -&gt; List[List[RetrievalResult]]</code>: For each query and its candidates, compute new scores, sort the results, and truncate to the top <code>k</code>.</li> </ul>"},{"location":"onboarding/#assembling-a-pipeline","title":"Assembling a Pipeline","text":"<p>Once you have created your custom component, you can use it in a <code>Searcher</code> pipeline.</p> <pre><code>from darelabdb.nlp_retrieval.searcher import Searcher\nfrom darelabdb.nlp_retrieval.loaders.jsonl_loader import JsonlLoader\nfrom darelabdb.nlp_retrieval.retrievers.dense_retriever import FaissRetriever\n# Import your new custom component\nfrom darelabdb.nlp_retrieval.rerankers.my_reranker import MyReranker\n\n# 1. Initialize the components\nmy_loader = JsonlLoader(file_path=\"path/to/data.jsonl\", content_field=\"text\")\nmy_retriever = FaissRetriever(model_name_or_path=\"BAAI/bge-m3\")\nmy_custom_reranker = MyReranker(model_name=\"some-model\") # Your new reranker\n\n# 2. Assemble the Searcher\nsearcher = Searcher(\n    retrievers=[my_retriever],\n    reranker=my_custom_reranker\n)\n\n# 3. Run the pipeline\nINDEX_DIR = \"./my_index\"\nsearcher.index(loader=my_loader, output_path=INDEX_DIR)\n\n# 4. Search\nqueries = [\"what is the capital of france?\", \"what is the best gpu for gaming?\"]\nresults = searcher.search(nlqs=queries, output_path=INDEX_DIR, k=5)\n\nprint(results)\n</code></pre>"},{"location":"onboarding/#4-part-2-evaluating-your-new-component","title":"4. Part 2: Evaluating Your New Component","text":"<p>After creating a new component, the next step is to measure its performance. The framework provides tools to run automated benchmarks and calculate standard information retrieval metrics.</p> <p> Figure 3: The benchmarking pipeline showing the complete flow from configuration through evaluation and logging.</p>"},{"location":"onboarding/#key-evaluation-classes","title":"Key Evaluation Classes","text":"<ul> <li><code>RetrievalEvaluator</code>: The core engine for calculating metrics like Precision, Recall, and F1-Score.</li> <li><code>Benchmarker</code>: The high-level orchestrator that runs different Searcher configurations, calls the RetrievalEvaluator, and logs results to the console and Weights &amp; Biases.</li> </ul>"},{"location":"onboarding/#the-matching-logic","title":"The Matching Logic","text":"<p>This is the most important concept in our evaluation system. A predicted item is considered a \"correct hit\" if its metadata is a superset of a gold standard item's metadata.</p> <p>Example:</p> <ul> <li>Gold Standard Metadata: <code>{'page_title': 'Mashable'}</code></li> <li>Prediction Metadata: <code>{'page_title': 'Mashable', 'source': 'some_sentence'}</code> \u2192 MATCH</li> </ul> <p>The evaluator also automatically handles granularity. It inspects the metadata keys in your gold standard to determine the level of uniqueness (e.g., by <code>page_title</code>, or by <code>page_title</code> and <code>source</code>). It then deduplicates predictions at that level before calculating metrics.</p>"},{"location":"onboarding/#how-to-run-a-benchmark","title":"How to Run a Benchmark","text":"<p>The <code>run_benchmark.py</code> script provides a template for setting up and running an evaluation.</p>"},{"location":"onboarding/#step-1-prepare-the-corpus-loader","title":"Step 1: Prepare the Corpus Loader","text":"<p>The Benchmarker requires an initialized loader object that returns a <code>List[SearchableItem]</code>.</p> <pre><code>from darelabdb.nlp_retrieval.loaders.jsonl_loader import JsonlLoader\n\nloader = JsonlLoader(file_path=\"path/to/corpus.jsonl\", ...)\n</code></pre>"},{"location":"onboarding/#step-2-prepare-queries-and-gold-standard","title":"Step 2: Prepare Queries and Gold Standard","text":"<p>You must create two lists in memory:</p> <ul> <li><code>queries: List[str]</code>: A list of query strings.</li> <li><code>gold_standard: List[List[RetrievalResult]]</code>: A parallel list where each inner list contains the correct <code>RetrievalResult</code> objects for the corresponding query. For the gold standard, only the metadata field is used for matching.</li> </ul>"},{"location":"onboarding/#step-3-configure-your-searcher-pipelines","title":"Step 3: Configure Your Searcher Pipelines","text":"<p>Create an initialized <code>Searcher</code> instance for each configuration you want to test, including your new component.</p> <pre><code># Configuration 1: Baseline\nbm25_searcher = Searcher(retrievers=[PyseriniRetriever()])\n\n# Configuration 2: Your new component\nmy_new_searcher = Searcher(retrievers=[MyNewRetriever()])\n\n# Create a list of named configurations for the Benchmarker\nsearcher_configs = [\n    (\"BM25_Baseline\", bm25_searcher),\n    (\"My_New_Retriever\", my_new_searcher),\n]\n</code></pre>"},{"location":"onboarding/#step-4-initialize-and-run-the-benchmarker","title":"Step 4: Initialize and Run the Benchmarker","text":"<p>Pass all the prepared objects to the Benchmarker and call <code>.run()</code>.</p> <pre><code>from darelabdb.nlp_retrieval.benchmarking.benchmarker import Benchmarker\nfrom darelabdb.nlp_retrieval.evaluation.evaluator import RetrievalEvaluator\n\nevaluator = RetrievalEvaluator()\nbenchmarker = Benchmarker(\n    searcher_configs=searcher_configs,\n    evaluator=evaluator,\n    loader=loader,\n    queries=queries,\n    gold_standard=gold_standard,\n    k_values=[1, 5, 10],\n    output_path=\"./benchmark_results\",\n    use_wandb=True,\n    wandb_project=\"my-retrieval-project\",\n    wandb_entity=\"my-username\",\n)\n\nbenchmarker.run()\n</code></pre>"},{"location":"onboarding/#interpreting-the-output","title":"Interpreting the Output","text":"<ul> <li>Console Output: The script will print progress, search speed (QPS), and a final summary table comparing all runs.</li> <li>Weights &amp; Biases (W&amp;B) Dashboard: If <code>use_wandb=True</code>, the Benchmarker logs detailed configurations and metrics for each run, along with an aggregated summary table, making it easy to compare pipeline performance.</li> </ul>"},{"location":"openapi/","title":"OpenAPI Specification","text":""},{"location":"openapi/#datagems-cross-dataset-discovery-service-v1","title":"DataGEMS Cross-Dataset Discovery Service v1","text":"<p>The Cross-Dataset Discovery Service provides a secure API for performing natural language search queries across a pre-indexed collection of datasets.</p> Terms of service: https://datagems.eu/terms Contact: DataGEMS helpdesk  License: EUPL-1.2 license"},{"location":"openapi/#servers","title":"Servers","text":"Description URL Default server path when running behind a reverse proxy /cdd"},{"location":"openapi/#monitoring","title":"Monitoring","text":""},{"location":"openapi/#get-health","title":"GET /health","text":"<p>Health Check</p> Description <p>Verifies the operational status of the API and its dependencies (Qdrant, TEI, Database). Returns a 200 OK if all systems are healthy.</p> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"message\": \"All dependencies are healthy.\",\n    \"status\": \"ok\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"message\": {\n            \"example\": \"All dependencies are healthy.\",\n            \"type\": \"string\"\n        },\n        \"status\": {\n            \"example\": \"ok\",\n            \"type\": \"string\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 424 Failed Dependency </p> application/json <p><pre><code>{\n    \"code\": 104,\n    \"error\": \"error communicating with underpinning service\",\n    \"message\": {\n        \"correlationId\": \"string\",\n        \"payload\": {},\n        \"source\": \"string\",\n        \"statusCode\": 0\n    }\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"example\": 104,\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"example\": \"error communicating with underpinning service\",\n            \"type\": \"string\"\n        },\n        \"message\": {\n            \"$ref\": \"#/components/schemas/FailedDependencyMessage\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#search","title":"Search","text":""},{"location":"openapi/#post-search","title":"POST /search/","text":"<p>Perform a Search Query</p> Description <p>Submits a natural language query and returns a ranked list of relevant results. The search can be optionally filtered to a specific set of datasets. Results are always filtered based on the user's access permissions.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>OAuth2Bearer</code> header string N/A No JWT token for authentication, obtained from the OIDC provider. <p>Request body</p> application/json <p><pre><code>{\n    \"dataset_ids\": [\n        \"string\"\n    ],\n    \"k\": 0,\n    \"query\": \"string\",\n    \"search_mode\": \"sparse\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"properties\": {\n        \"dataset_ids\": {\n            \"description\": \"A list of dataset UUIDs to restrict the search to.\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"nullable\": true,\n            \"type\": \"array\"\n        },\n        \"k\": {\n            \"default\": 5,\n            \"description\": \"The number of results to return.\",\n            \"maximum\": 100,\n            \"minimum\": 1,\n            \"type\": \"integer\"\n        },\n        \"query\": {\n            \"description\": \"The natural language query string.\",\n            \"type\": \"string\"\n        },\n        \"search_mode\": {\n            \"default\": \"sparse\",\n            \"description\": \"The retrieval strategy to use.\",\n            \"enum\": [\n                \"sparse\",\n                \"dense\",\n                \"hybrid\"\n            ],\n            \"type\": \"string\"\n        }\n    },\n    \"required\": [\n        \"query\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"query_time\": 10.12,\n    \"results\": [\n        {\n            \"content\": \"string\",\n            \"dataset_id\": \"string\",\n            \"object_id\": \"string\",\n            \"similarity\": 10.12\n        }\n    ]\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"query_time\": {\n            \"description\": \"The time taken to process the query in milliseconds.\",\n            \"format\": \"float\",\n            \"type\": \"number\"\n        },\n        \"results\": {\n            \"items\": {\n                \"$ref\": \"#/components/schemas/API_SearchResult\"\n            },\n            \"type\": \"array\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 400 Bad Request </p> application/json <p><pre><code>{\n    \"code\": 102,\n    \"error\": \"Validation Error\",\n    \"message\": [\n        {\n            \"Key\": \"string\",\n            \"Value\": [\n                \"string\"\n            ]\n        }\n    ]\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"example\": 102,\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"example\": \"Validation Error\",\n            \"type\": \"string\"\n        },\n        \"message\": {\n            \"items\": {\n                \"$ref\": \"#/components/schemas/ValidationErrorDetail\"\n            },\n            \"type\": \"array\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 401 Unauthorized </p> <p> Response 403 Forbidden </p> <p> Response 424 Failed Dependency </p> application/json <p><pre><code>{\n    \"code\": 104,\n    \"error\": \"error communicating with underpinning service\",\n    \"message\": {\n        \"correlationId\": \"string\",\n        \"payload\": {},\n        \"source\": \"string\",\n        \"statusCode\": 0\n    }\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"example\": 104,\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"example\": \"error communicating with underpinning service\",\n            \"type\": \"string\"\n        },\n        \"message\": {\n            \"$ref\": \"#/components/schemas/FailedDependencyMessage\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#schemas","title":"Schemas","text":""},{"location":"openapi/#api_searchresult","title":"API_SearchResult","text":"Name Type <code>content</code> string <code>dataset_id</code> string <code>object_id</code> string <code>similarity</code> number(float)"},{"location":"openapi/#faileddependencymessage","title":"FailedDependencyMessage","text":"Name Type <code>correlationId</code> string| null <code>payload</code> <code>source</code> string <code>statusCode</code> integer"},{"location":"openapi/#faileddependencyresponse","title":"FailedDependencyResponse","text":"Name Type <code>code</code> integer <code>error</code> string <code>message</code> FailedDependencyMessage"},{"location":"openapi/#searchrequest","title":"SearchRequest","text":"Name Type <code>dataset_ids</code> Array&lt;string&gt; <code>k</code> integer <code>query</code> string <code>search_mode</code> string"},{"location":"openapi/#searchresponse","title":"SearchResponse","text":"Name Type <code>query_time</code> number(float) <code>results</code> Array&lt;API_SearchResult&gt;"},{"location":"openapi/#validationerrordetail","title":"ValidationErrorDetail","text":"Name Type <code>Key</code> string <code>Value</code> Array&lt;string&gt;"},{"location":"openapi/#validationerrorresponse","title":"ValidationErrorResponse","text":"Name Type <code>code</code> integer <code>error</code> string <code>message</code> Array&lt;ValidationErrorDetail&gt;"},{"location":"openapi/#security-schemes","title":"Security schemes","text":"Name Type Scheme Description OAuth2Bearer http bearer JWT token for authentication, obtained from the OIDC provider."},{"location":"openapi/#tags","title":"Tags","text":"Name Description Search Endpoints for performing search operations. Monitoring Endpoints for monitoring the service health."},{"location":"qa/","title":"Quality Assurance","text":"<p>Quality assurance (QA) combines automated static analysis, vulnerability scanning, and API-level integration testing to ensure the service functions as expected.</p>"},{"location":"qa/#code-analysis","title":"Code Analysis","text":"<p>We use GitHub CodeQL for static analysis of the source code. This process helps identify potential bugs, security vulnerabilities, and quality issues before they impact production.</p> <ul> <li>Technology: GitHub CodeQL</li> <li>Targets: The analysis runs on both the Python source code and the GitHub Actions workflow files themselves.</li> <li>Checks: It uses a set of queries to find common vulnerabilities (e.g., injection flaws, insecure data handling), bugs, and code quality anti-patterns.</li> <li>Execution: The analysis is performed by the on-demand <code>code-scan-on-demand.yml</code> workflow. Results and alerts are available directly in the repository's \"Security\" tab.</li> </ul>"},{"location":"qa/#code-metrics","title":"Code Metrics","text":"<p>To ensure the long-term maintainability and readability of the code, we use the Radon library to generate code metrics.</p> <ul> <li>Technology: Radon</li> <li>Metrics:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain (higher is better).</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code. A lower score indicates simpler, less complex code.</li> <li>Lines of Code (LOC): Provides raw metrics on the size of the codebase.</li> </ul> </li> <li>Execution: The metrics are generated by the on-demand <code>code-metrics-on-demand.yml</code> workflow, which produces a downloadable <code>code-metrics-report.txt</code> artifact.</li> </ul>"},{"location":"qa/#vulnerability-checks","title":"Vulnerability Checks","text":"<p>We use Trivy to scan for known vulnerabilities in our dependencies and container image, ensuring the service is secure from common threats.</p> <ul> <li>Technology: Trivy</li> <li>Scans:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository files for security misconfigurations.</li> <li>Image Scan: Scans the final Docker image for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity in its OS packages and Python libraries.</li> </ol> </li> <li>Execution: The scan is performed by the on-demand <code>vulnerability-scan-on-demand.yml</code> workflow. It requires an image tag as input and uploads detailed JSON reports as build artifacts.</li> </ul>"},{"location":"qa/#testing","title":"Testing","text":"<p>The service's functionality is validated through API-level integration tests using Postman and its command-line runner, Newman.</p> <ul> <li> <p>Test Suite: The test cases are defined in the Postman collection located at <code>tests/cross-dataset-discovery-api-tests.postman_collection.json</code>.</p> </li> <li> <p>Test Cases: The suite includes the following tests:</p> <ul> <li>Health Check: Verifies that the <code>/health</code> endpoint is available and returns a <code>200 OK</code> status, indicating the service and its dependencies are healthy.</li> <li>Get User Access Token: Simulates a user login against the OIDC provider to acquire a valid JWT, which is required for all protected endpoints.</li> <li>Perform Search - Valid Request: Executes a valid search against the <code>/search/</code> endpoint and asserts that the response is successful (<code>200 OK</code>) and has the correct structure.</li> <li>Perform Search - Bad Request: Sends a request with an invalid payload (an empty <code>dataset_ids</code> list) and asserts that the API correctly rejects it with a <code>400 Bad Request</code> status.</li> </ul> </li> </ul>"},{"location":"qa/#how-to-run-tests","title":"How to Run Tests","text":"<p>The API tests are designed to be run automatically via a GitHub Actions workflow.</p> <ul> <li>Workflow File: <code>.github/workflows/test-on-demand.yml</code></li> <li>Trigger: The workflow is triggered manually (<code>workflow_dispatch</code>) from the Actions tab in the GitHub repository.</li> <li>Process:<ol> <li>Navigate to the \"Actions\" tab and select the \"Test Scenario (On-Demand)\" workflow.</li> <li>Click \"Run workflow\". You will be prompted to enter a <code>tag</code> (e.g., <code>main</code> or a specific version like <code>v1.0.0</code>) to test against.</li> <li>The workflow checks out the specified version of the code.</li> <li>It then uses the official <code>postman/newman</code> Docker image to execute the test collection.</li> <li>All necessary environment variables (API base URL, credentials, etc.) are securely injected into the test run from the repository's secrets.</li> </ol> </li> </ul>"},{"location":"qa/#expected-output","title":"Expected Output","text":"<p>A successful test run will produce the following summary table in the GitHub Actions log:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502            executed \u2502             failed \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              iterations \u2502                   1 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                requests \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            test-scripts \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      prerequest-scripts \u2502                   0 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              assertions \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total run duration: 1674ms                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total data received: 6.9kB (approx)                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average response time: 396ms [min: 147ms, max: 645ms, s.d.: 205ms] \u2502\n</code></pre>"},{"location":"security/","title":"Security","text":""},{"location":"security/#authentication","title":"Authentication","text":"<p>Authentication is handled via the OAuth 2.0 and OpenID Connect (OIDC) protocols. Every request to a protected endpoint must include a valid JSON Web Token (JWT) in the <code>Authorization</code> header as a Bearer token.</p> <p>The service validates incoming JWTs against the OIDC provider's public keys. The token's signature, issuer, and audience are all verified to ensure its authenticity.</p>"},{"location":"security/#authorization","title":"Authorization","text":"<p>Authorization is role-based. The service checks for specific roles within the validated JWT's claims.</p> <ul> <li>Required Roles: To access the <code>/search/</code> endpoint, the user's token must contain at least one of the following roles in its <code>realm_access.roles</code> claim:<ul> <li><code>user</code></li> <li><code>dg_user</code></li> </ul> </li> </ul> <p>If a user attempts to access an endpoint without the required role, the API will respond with a <code>403 Forbidden</code> error.</p>"},{"location":"security/#dataset-level-permissions","title":"Dataset-Level Permissions","text":"<p>In addition to role-based access, the service enforces dataset-level permissions.</p> <p>Before executing a search, the API performs a token exchange and then calls the DataGEMS Gateway's <code>POST /api/principal/context-grants/query</code> endpoint. It requests all grants associated with roles like <code>dg_ds-browse</code> to get a list of all dataset IDs the user is permitted to access.</p> <ul> <li>Search Filtering:<ul> <li>If a user's search request includes <code>dataset_ids</code>, the service intersects this list with the user's authorized datasets. The search is only performed on the datasets present in both lists.</li> <li>All search results, regardless of the input filter, are always filtered against the user's authorized dataset list before being returned. This ensures a user can never see data from a dataset they are not permitted to access.</li> </ul> </li> </ul>"}]}